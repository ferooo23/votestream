# app/main.py

import json
import asyncio
import time
import hashlib
import logging
from fastapi import FastAPI, WebSocket, Depends, HTTPException, status, WebSocketDisconnect, Request
from sqlmodel import SQLModel, Session, select, create_engine
from sqlalchemy import func, insert
from sqlalchemy.orm import selectinload
from sqlalchemy.pool import QueuePool
import redis as redis_py
from typing import Optional, List, Dict, Set
from datetime import datetime, timedelta
from contextlib import asynccontextmanager
from fastapi.middleware.cors import CORSMiddleware
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.responses import JSONResponse
from enum import Enum

from models import Poll, Option, Vote, PollOptionLink

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

DATABASE_URL = "postgresql://postgres:pass@db:5432/votes"
REDIS_URL    = "redis://redis:6379/0"

# Circuit breaker implementation for enhanced resilience
class CircuitState(Enum):
    CLOSED = "closed"
    OPEN = "open" 
    HALF_OPEN = "half_open"

class CircuitBreaker:
    """
    Circuit breaker pattern implementation for database resilience.
    Prevents cascade failures when database is under stress.
    """
    def __init__(self, failure_threshold: int = 5, timeout: int = 60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED
    
    def call(self, func, *args, **kwargs):
        if self.state == CircuitState.OPEN:
            if self._should_attempt_reset():
                self.state = CircuitState.HALF_OPEN
            else:
                raise HTTPException(
                    status_code=503, 
                    detail="Service temporarily unavailable - Circuit breaker open"
                )
        
        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()
            raise e
    
    def _should_attempt_reset(self):
        return (
            self.last_failure_time and 
            time.time() - self.last_failure_time >= self.timeout
        )
    
    def _on_success(self):
        self.failure_count = 0
        self.state = CircuitState.CLOSED
    
    def _on_failure(self):
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            self.state = CircuitState.OPEN

# Global circuit breaker instances
db_circuit_breaker = CircuitBreaker(failure_threshold=5, timeout=60)
redis_circuit_breaker = CircuitBreaker(failure_threshold=3, timeout=30)

# Rate limiting middleware
class RateLimitMiddleware(BaseHTTPMiddleware):
    def __init__(self, app, calls: int = 10, period: int = 60):
        super().__init__(app)
        self.calls = calls
        self.period = period

    async def dispatch(self, request: Request, call_next):
        # Get client IP
        client_ip = request.client.host
        
        # Create unique client key
        key = f"rate_limit:{client_ip}"
        
        # Check rate limit
        current_time = time.time()
        try:
            # Use Redis for rate limiting
            pipe = redis_client.pipeline()
            pipe.zremrangebyscore(key, 0, current_time - self.period)
            pipe.zcount(key, current_time - self.period, current_time)
            pipe.zadd(key, {str(current_time): current_time})
            pipe.expire(key, self.period)
            results = pipe.execute()
            
            current_calls = results[1]
            
            if current_calls >= self.calls:
                return JSONResponse(
                    status_code=429,
                    content={"detail": "Rate limit exceeded"}
                )
        except Exception as e:
            logger.warning(f"Rate limiting error: {e}")
            # If rate limiting fails, allow the request
            pass
        
        response = await call_next(request)
        return response

# Optimize database connections
engine = create_engine(
    DATABASE_URL, 
    echo=False,  # Disable SQL logs in production
    pool_size=20,  # Increase pool size
    max_overflow=30,
    pool_pre_ping=True,
    pool_recycle=3600,
    poolclass=QueuePool
)

# Configure Redis with connection pooling
redis_client = redis_py.from_url(
    REDIS_URL, 
    decode_responses=True,
    max_connections=20,
    retry_on_timeout=True,
    socket_timeout=5
)

# Create tables
SQLModel.metadata.create_all(engine)

# Active WebSocket connections
active_connections: Dict[str, Set[WebSocket]] = {}

# Lifespan to clean connections
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("Application started")
    yield
    # Shutdown
    logger.info("Closing WebSocket connections...")
    for poll_id, connections in active_connections.items():
        for ws in connections.copy():
            try:
                await ws.close()
            except:
                pass
        connections.clear()
    logger.info("Application closed")

app = FastAPI(title="Vote Stream", lifespan=lifespan)

# Add CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Add rate limiting
app.add_middleware(RateLimitMiddleware, calls=100, period=60)


def get_session():
    with Session(engine) as session:
        yield session


@app.get("/health")
def health():
    try:
        # Check Redis
        redis_client.ping()
        redis_ok = True
    except:
        redis_ok = False
    
    try:
        # Check DB
        with Session(engine) as session:
            session.exec(select(func.count(Poll.id)))
        db_ok = True
    except:
        db_ok = False
    
    status_value = "ok" if redis_ok and db_ok else "degraded"
    
    return {
        "status": status_value,
        "redis": redis_ok,
        "database": db_ok,
        "active_connections": len(active_connections)
    }


# â€” Schemas for output â€”
class OptionRead(SQLModel):
    id: int
    text: str

class PollRead(SQLModel):
    id: int
    question: str
    theme: Optional[str] = None
    options: List[OptionRead]

class PollWithResults(SQLModel):
    id: int
    question: str
    theme: Optional[str] = None
    options: List[OptionRead]
    results: Dict[str, int]


# 1) Cached aggregated results
@app.get("/polls/{poll_id}/results")
def results(poll_id: int, session: Session = Depends(get_session)):
    key = f"results:{poll_id}"
    cached = redis_client.get(key)
    if cached:
        return json.loads(cached)
    
    # Optimized query with index hints
    stmt = (
        select(Option.text, func.count(Vote.id).label('vote_count'))
        .join(Vote, Vote.option_id == Option.id, isouter=True)
        .join(PollOptionLink, PollOptionLink.option_id == Option.id)
        .where(PollOptionLink.poll_id == poll_id)
        .group_by(Option.id, Option.text)
    )
    rows = session.exec(stmt).all()
    res = {text: count for text, count in rows}
    
    # Cache for longer time if no votes yet
    cache_time = 300 if any(count > 0 for count in res.values()) else 60
    redis_client.set(key, json.dumps(res), ex=cache_time)
    return res


# 2) Cached single poll read
@app.get("/polls/{poll_id}", response_model=PollRead)
def read_poll(poll_id: int, session: Session = Depends(get_session)):
    key = f"poll:{poll_id}"
    raw = redis_client.get(key)
    if raw:
        return PollRead.parse_raw(raw)
    poll = session.exec(
        select(Poll).options(selectinload(Poll.options))
        .where(Poll.id == poll_id)
    ).one_or_none()
    if not poll:
        raise HTTPException(status_code=404, detail="Poll not found")
    pr = PollRead.from_orm(poll)
    redis_client.set(key, pr.json(), ex=60)
    return pr


# 3) Combined endpoint: polls + their current results
@app.get("/polls-with-results", response_model=List[PollWithResults])
def list_polls_with_results(session: Session = Depends(get_session)):
    polls = session.exec(
        select(Poll).options(selectinload(Poll.options))
    ).all()
    out: List[PollWithResults] = []
    for p in polls:
        pr = PollRead.from_orm(p)
        rr = results(p.id, session)  # uses cache
        out.append(PollWithResults(
            id=pr.id,
            question=pr.question,
            options=pr.options,
            results=rr
        ))
    return out


# 4) List polls (with options)
@app.get("/polls", response_model=List[PollRead])
def list_polls(
    active: Optional[bool] = None,
    theme: Optional[str] = None,
    skip: int = 0,
    limit: int = 100,
    session: Session = Depends(get_session)
):
    query = select(Poll).options(selectinload(Poll.options))
    if active is not None:
        query = query.where(Poll.is_active == active)
    if theme is not None:
        query = query.where(Poll.theme == theme)
    polls = session.exec(query.offset(skip).limit(limit)).all()
    return [PollRead.from_orm(p) for p in polls]


# 4.1) Get polls by theme
@app.get("/themes/{theme}/polls", response_model=List[PollRead])
def get_polls_by_theme(
    theme: str,
    session: Session = Depends(get_session)
):
    query = select(Poll).options(selectinload(Poll.options)).where(Poll.theme == theme)
    polls = session.exec(query).all()
    return [PollRead.from_orm(p) for p in polls]


# 4.2) Get available themes
@app.get("/themes")
def get_themes(session: Session = Depends(get_session)):
    themes_info = {
        "OnTrend": {
            "name": "#OnTrend",
            "slug": "on-trend",
            "description": "The hottest topics of the moment. From technology and social media to the releases everyone is talking about. Vote and discover if you think like the majority!",
            "color": "#ff6b6b",
            "icon": "ðŸ”¥"
        },
        "MoralDilemmas": {
            "name": "Moral Dilemmas",
            "slug": "moral-dilemmas",
            "description": "Difficult questions with no right answer. Test your principles and discover how the world would react to these extreme situations.",
            "color": "#4ecdc4",
            "icon": "ðŸ¤”"
        },
        "Sports": {
            "name": "âš½ Sports",
            "slug": "sports",
            "description": "The boldest predictions in the world of sports. Will you get your predictions right?",
            "color": "#45b7d1",
            "icon": "âš½"
        }
    }
    
    # Add poll count by theme
    for theme_key in themes_info.keys():
        count = session.exec(
            select(func.count(Poll.id)).where(Poll.theme == theme_key)
        ).one()
        themes_info[theme_key]["poll_count"] = count
    
    return themes_info


# 5) Vote and publish via Redis (invalidate cache) - OPTIMIZED
@app.post("/polls/{poll_id}/vote")
async def vote(
    poll_id: int,
    payload: dict,
    session: Session = Depends(get_session)
):
    choice_idx = payload.get("choice")
    
    # Validate basic input
    if not isinstance(choice_idx, int) or choice_idx < 0:
        raise HTTPException(status_code=400, detail="Invalid choice")
    
    # Cache key for the poll
    poll_key = f"poll:{poll_id}"
    
    # Try to get poll from cache first
    cached_poll = redis_client.get(poll_key)
    if cached_poll:
        try:
            poll_data = json.loads(cached_poll)
            if choice_idx >= len(poll_data.get("options", [])):
                raise HTTPException(status_code=400, detail="Invalid choice")
            option_id = poll_data["options"][choice_idx]["id"]
        except (json.JSONDecodeError, KeyError):
            # Fallback to DB if cache is corrupted
            poll = session.exec(
                select(Poll).options(selectinload(Poll.options))
                .where(Poll.id == poll_id)
            ).one_or_none()
            if not poll or choice_idx >= len(poll.options):
                raise HTTPException(status_code=400, detail="Invalid poll or choice")
            option_id = poll.options[choice_idx].id
    else:
        # Get from DB
        poll = session.exec(
            select(Poll).options(selectinload(Poll.options))
            .where(Poll.id == poll_id)
        ).one_or_none()
        if not poll or choice_idx >= len(poll.options):
            raise HTTPException(status_code=400, detail="Invalid poll or choice")
        option_id = poll.options[choice_idx].id

    # Insert vote in optimized way
    vote_obj = Vote(poll_id=poll_id, option_id=option_id)
    session.add(vote_obj)
    session.commit()

    # Invalidate results cache
    redis_client.delete(f"results:{poll_id}")

    # Publish updated results asynchronously
    try:
        res = results(poll_id, session)
        redis_client.publish(f"poll_{poll_id}", json.dumps(res))
    except Exception as e:
        logger.error(f"Error publishing results: {e}")
    
    return {"status": "ok"}


# 6) WebSocket for real-time updates (non-blocking)
@app.websocket("/polls/{poll_id}/stream")
async def stream(ws: WebSocket, poll_id: int):
    await ws.accept()
    
    # Add connection to pool
    poll_key = str(poll_id)
    if poll_key not in active_connections:
        active_connections[poll_key] = set()
    active_connections[poll_key].add(ws)
    
    pubsub = redis_client.pubsub(ignore_subscribe_messages=True)
    pubsub.subscribe(f"poll_{poll_id}")
    
    try:
        while True:
            msg = pubsub.get_message(timeout=1.0)
            if msg and msg["type"] == "message":
                await ws.send_text(msg["data"])
            await asyncio.sleep(0.1)
    except WebSocketDisconnect:
        logger.info(f"WebSocket disconnected for poll {poll_id}")
    except Exception as e:
        logger.error(f"WebSocket error for poll {poll_id}: {e}")
    finally:
        # Clean connection
        if poll_key in active_connections:
            active_connections[poll_key].discard(ws)
            if not active_connections[poll_key]:
                del active_connections[poll_key]
        pubsub.close()
        try:
            await ws.close()
        except:
            pass


# 7) Create new poll
class OptionCreate(SQLModel):
    text: str

class PollCreate(SQLModel):
    question: str
    theme: Optional[str] = None
    options: List[OptionCreate]

@app.post("/polls", response_model=PollRead, status_code=status.HTTP_201_CREATED)
def create_poll(
    poll_in: PollCreate,
    session: Session = Depends(get_session)
):
    poll = Poll(question=poll_in.question, theme=poll_in.theme)
    session.add(poll)
    session.commit()
    session.refresh(poll)

    for opt_in in poll_in.options:
        opt = Option(text=opt_in.text)
        session.add(opt)
        session.commit()
        session.refresh(opt)
        
        # Create poll-option link
        link = PollOptionLink(poll_id=poll.id, option_id=opt.id)
        session.add(link)

    session.commit()

    # Invalidate poll cache
    redis_client.delete(f"poll:{poll.id}")

    # Reload with options
    poll = session.exec(
        select(Poll).options(selectinload(Poll.options))
        .where(Poll.id == poll.id)
    ).one()
    return PollRead.from_orm(poll)


# 8) General stats with caching
@app.get("/stats")
def get_stats(session: Session = Depends(get_session)):
    key = "stats:general"
    cached = redis_client.get(key)
    if cached:
        return json.loads(cached)
    
    total_polls      = session.exec(select(func.count(Poll.id))).one()
    total_votes      = session.exec(select(func.count(Vote.id))).one()
    polls_with_votes = session.exec(
        select(func.count(func.distinct(Vote.poll_id)))
    ).one()
    
    stats = {
        "total_polls": total_polls,
        "total_votes": total_votes,
        "polls_with_votes": polls_with_votes
    }
    
    # Cache stats for 5 minutes
    redis_client.set(key, json.dumps(stats), ex=300)
    return stats


# 9) Advanced metrics for scalability monitoring
@app.get("/metrics")
def get_metrics(session: Session = Depends(get_session)):
    """
    Enhanced metrics endpoint for monitoring scalability and performance.
    Demonstrates advanced monitoring capabilities for scalable systems.
    """
    key = "metrics:advanced"
    cached = redis_client.get(key)
    if cached:
        return json.loads(cached)
    
    try:
        # Database metrics
        total_polls = session.exec(select(func.count(Poll.id))).one()
        total_votes = session.exec(select(func.count(Vote.id))).one()
        total_options = session.exec(select(func.count(Option.id))).one()
        
        # Performance metrics
        polls_with_votes = session.exec(
            select(func.count(func.distinct(Vote.poll_id)))
        ).one()
        
        # Calculate average options per poll safely
        avg_options_query = session.exec(
            select(func.avg(func.count(Option.id)))
            .select_from(Option)
            .join(PollOptionLink)
            .group_by(PollOptionLink.poll_id)
        ).first()
        avg_options_per_poll = float(avg_options_query) if avg_options_query else 0.0
        
        # Recent activity (last 24 hours)
        yesterday = datetime.utcnow() - timedelta(days=1)
        recent_votes = session.exec(
            select(func.count(Vote.id))
            .where(Vote.voted_at >= yesterday)
        ).one()
        
        # System health metrics
        active_ws_connections = sum(len(conns) for conns in active_connections.values())
        
        # Redis info
        redis_info = redis_client.info()
        redis_memory_used = redis_info.get('used_memory_human', 'N/A')
        redis_connected_clients = redis_info.get('connected_clients', 0)
        
        metrics = {
            "database": {
                "total_polls": total_polls,
                "total_votes": total_votes,
                "total_options": total_options,
                "polls_with_votes": polls_with_votes,
                "avg_options_per_poll": round(float(avg_options_per_poll), 2),
                "votes_per_poll_avg": round(total_votes / max(total_polls, 1), 2)
            },
            "performance": {
                "recent_votes_24h": recent_votes,
                "vote_rate_per_hour": round(recent_votes / 24, 2),
                "active_websocket_connections": active_ws_connections,
                "polls_participation_rate": round((polls_with_votes / max(total_polls, 1)) * 100, 2)
            },
            "system": {
                "redis_memory_used": redis_memory_used,
                "redis_connected_clients": redis_connected_clients,
                "app_active_polls": len(active_connections),
                "timestamp": datetime.utcnow().isoformat()
            },
            "scalability_indicators": {
                "horizontal_scaling_ready": True,
                "cache_hit_potential": "high" if total_votes > 100 else "medium",
                "load_distribution": "optimal" if active_ws_connections < 1000 else "monitor"
            }
        }
        
        # Cache metrics for 30 seconds (real-time but not overwhelming)
        redis_client.set(key, json.dumps(metrics), ex=30)
        return metrics
        
    except Exception as e:
        logger.error(f"Error generating metrics: {e}")
        return {
            "error": "Could not generate metrics",
            "basic_stats": {
                "active_connections": len(active_connections),
                "timestamp": datetime.utcnow().isoformat()
            }
        }
